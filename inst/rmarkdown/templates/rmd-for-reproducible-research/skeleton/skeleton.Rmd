---
title: "Reproducible Research"
authort: 
data: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


```{r packages, echo=FALSE}
suppressPackageStartupMessages(suppressWarnings({
  library(ipumsr)
  library(tidyverse)
}))

```


# {.tabset}

## Setup - First Time

This script is intended to semi-automate parts of the IPUMS data acquisition process, using the [microdata API]().  In order to run, users must: 
1. Have an account with one or more IPUMS project
1. Register for an [IPUMS microdata API key](https://developer.ipums.org/docs/apiprogram/)
1. Add your API key as an [environmental variable](https://tech.popdata.org/ipumsr/dev/)


Some Notes on this file:
This script uses `{.tabset}`s and `code-folding` in an attempt to keep the back-end setup and user-facing analysis separate, more organized, and hopefully easier to work with. 

This script is set up with the intention of being shared, either directly or via github. When sharing,  **do not** share the **data** (`.dat.gz`) or **metadata** (`.xml`) files. This script will automatically download those files for the user using the `.json` **extract definition**. If using github, be sure to add the `.dat.gz` and `.xml` files to `.gitignore`, or store them outside the git repo.


## Setup - Project Parameters

This template makes it easy for users to share their custom IPUMS data extracts, without directly sharing microdata. 
* When starting an analysis/project, it handles every step between "creating an extract online" and "analyzing the data". 
* When sharing the template with others, it will automatically submit, download, and compile the report for the end user in just 2 clicks - allowing you to share interactive, data driven reports with colleagues without needing to send massive data files.  

To get started, build your data extract via the Data Cart GUI on [ipums.org](www.ipums.org), and be sure to take note of which IPUMS **collection** you are using (EG, "USA", "CPS"), and which **extract number** you'd like to use. Enter them in the relevant `parameters` below.

Additionally, you'll need to specify a **descriptive name** to help keep track of your files and the **data directory** you'd like your files to go in. With those parameters in place, this script will:

1. Build a `.json` extract definition (if you don't already have one)
1. Submit/Check on extract
1. Download data and metadata to specified directory
1. Relabel files for consistency and legibility.

Fill in the 4 parameters below and click `knit` or run `rmarkdown::render()` to begin. 
* The first time the script will "fail" and inform you the data is not ready. Smaller extracts may be ready in just a few minutes, but extracts with many variables/samples may take longer.
  + If it wasn't already present, you should now see a `.json` file in the `data_dir`.
* Re-running (or re-knitting) the script will check on the status of the specified extract. If it is ready, it will automatically download to the **data_dir** and rename based on **descriptive_name**.

Read on for more info on parameters and the back-end code, or once your data is present, skip ahead to [## Analysis Awaits] to continue as usual. 


```{r project_paramaters}

#### Key Parameters #####

collection <- "usa"

extract_num <- ""

data_dir <- file.path("Data")

descriptive_name <- "template"

```

### Parameter Definitions

`collection` The IPUMS data **collection** to query, abbreviated and lower-case. Print `ipums_data_collections()` to see a list of all IPUMS projects and the api-specific name.
  + Note: Currently, only USA and CPS are supported.
  
`extract_num` Which extract from the above collection to use, integer only without leading 0s. Leave blank (`""`) for most recent extract:
  + `extract_num <- 42` For extract "0000042" 
  + `extract_num <- ""` For most recent
  

`data_dir` A **directory** to download your **data** (will be created if it does not exist). We recommend storing data, dictionaries, and extract definitions within a sub-folder. The default will create a sub-folder named "Data" within your R project. 
* If you want to store your files at the **top-level** of the project directory use: 
  + `data_dir <- file.path("")`.
* If you want to store your files **outside** of the project directory use:
  + `data_dir <- file.path("..","Data")` to store in a sibling-directory to the project directory.
  + The `".."` goes "up" one level within a folder system.

`descriptive_name` IPUMS provides numerical IDs for each data extract by default (eg, `usa_000001.dat.gz, usa_000001.xml`), however these are specific to individual users and can be confusing to keep track of. We recommend users relabel their extracts using a project-/analysis- specific **descriptive name**, eg: "prcs_migration_ex". The script will automatically apply the same `descriptive_name` to the `.json, .dat.gz, .xml`, as well as a `.csv` file used for checking extract status.

  
From here, you can skip ahead to [## Analysis Awaits].

#### Initial setup

```{r}


if(!data_dir==""){
if(!dir.exists(data_dir)){
  dir.create(data_dir)
}
}

if(collection==""){
  stop("Please specify a collection, one of c('usa', 'cps')", call. = F)
}

if(descriptive_name==""){
  stop("Please specify a descriptive name for files",call. = F)
}


json_filename <- paste0(descriptive_name,".json")
data_rename <- paste0(descriptive_name,".dat.gz")
ddi_rename <- paste0(descriptive_name,".xml")
chk_name <- paste0("chk_",descriptive_name,".csv")

json_present <- file.exists(
  file.path(data_dir, json_filename))


data_present <- file.exists(
  file.path(data_dir, data_rename)) &
  file.exists(file.path(data_dir, ddi_rename)
              )


submitted <- file.exists(file.path(data_dir, chk_name))

if(submitted){
submitted_num <- read.csv(file.path(data_dir, chk_name))[[1]]
}


if(json_present){
  input_json <- list.files(path = data_dir, pattern = ".json")


if(length(input_json) > 1){
  stop("Multiple .json definitions present, please use a separate data_dir for each .json",call. = F)
}

if(!identical(input_json, json_filename)){
  warning(paste("Updating .json from", input_json, "to", json_filename),call. = F)
  file.rename(file.path(data_dir, input_json),
              file.path(data_dir, json_filename))
}

}

```


The following handles the various sources of extract definitions, depending on input decisions, creating `extract_info`, which will be used to actually do the checking/downloading.

```{r, eval = !json_present}


if(is.numeric(extract_num)){

  extract_info <- get_extract_info(c(collection, extract_num)) 
  extract_info %>% 
  save_extract_as_json(file = file.path(data_dir,
                                        json_filename)
                       )
  
  write.csv(extract_info$number, file.path(data_dir, chk_name), row.names = F)
  stop(".json created, please re-run to check on data", call. = F)
  
} else if ( extract_num==""){

extract_info <- get_last_extract_info(collection)

  extract_info %>%
    save_extract_as_json(file = file.path(data_dir,
                                          json_filename)
                         )
  
   write.csv(extract_info$number, file.path(data_dir, chk_name), row.names = F)
  
  stop(".json created, please re-run to check on data",call. = F)

}


```

Once the `.json` is present, we begin the process of checking and submitting. `sumitted` is `TRUE` as long as the `chk_.csv` flag file is in the `data_dir`. If this is not present, but a .json is, the script will submit the extract for the first time, in the `else` section below. 
```{r, eval = json_present}
extract_info <- define_extract_from_json(file.path(data_dir, json_filename))


if(submitted) {
  ## read extract number
  extract_info$number <- submitted_num
  ## check on extract
  extract_info <- get_extract_info(extract_info)
  already_ready <- is_extract_ready(extract_info)
  
  ## stale request, need to re-submit
  stale <- (!already_ready) & extract_info$status == "completed"
  waiting <- already_ready & extract_info$status == "incomplete"
  
  ready <- already_ready & extract_info$status == "completed"
  
} else {
  ## submit for the first time
  extract_info <- extract_info %>% submit_extract()
  save_extract_as_json(extract_info, file.path(data_dir, json_filename))
  write.csv(extract_info$number,
            file.path(data_dir, chk_name),
            row.names = F)
  stop("Extract submitted to IPUMS. Please re-run in a few minutes to check on/download data.",call. = F)
  
}

```

#### Submit, Check on, Download Extract

IPUMS only ensures data will be available for 72 hours, after that point users will need to re-submit an extract request. If your data is out of date, this will re-submit for you. If the data are not ready, it will let you know to check back in a few, or if it is ready it will download BOTH the data and data dictionary to `data_dir` based on the `descriptive_name` 

```{r, eval = stale }

#### Depending on status of extract...

if ((!already_ready) & extract_info$status == "completed") {
  ## stale request, need to re-submit
  
  extract_info <- extract_info %>% submit_extract()
  save_extract_as_json(extract_info, file.path(data_dir, json_filename))
  write.csv(extract_info$number,
            file.path(data_dir, chk_name),
            row.names = F)
  stop("Specified extract expired, resubmitting, check back in a few mins", call. = F)
  
} else if (already_ready & extract_info$status == "incomplete") {
  ## data not ready print warning
  stop("Extract not ready. Please wait a few mins and re-run file.", call. = F)
} else if (already_ready & extract_info$status == "completed") {
  ## get data
  
  ddi_filename <- extract_info %>%
    download_extract(download_dir = data_dir) %>%
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names
  file.rename(file.path(data_dir, ddi_filename),
              file.path(data_dir, ddi_rename))
  file.rename(file.path(data_dir, data_filename),
              file.path(data_dir, data_rename))
}


```

#### Load Data

Now we're ready to begin analysis, and your project will be shareable/reproducible for other IPUMS users.

```{r eval = data_present}


ddi <- read_ipums_ddi(file.path(data_dir, ddi_rename))
data <- read_ipums_micro(ddi, data_file = file.path(data_dir, data_rename))

```



## Analysis Awaits {.active}

```{r, eval = data_present}

## Feel free to delete
print(extract_info$collection)
print(extract_info$description)

n_hh <- data %>% filter(perNum==0) %>% nrow()
n_per <- data %>% filter(perNum != 0) %>% nrow()
print(paste("This extract contains", n_hh, "household records and", n_per, "person records"))
```
